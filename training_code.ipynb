{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-09-29T16:03:15.235365Z","iopub.status.busy":"2023-09-29T16:03:15.235043Z","iopub.status.idle":"2023-09-29T16:03:15.240609Z","shell.execute_reply":"2023-09-29T16:03:15.239477Z","shell.execute_reply.started":"2023-09-29T16:03:15.235338Z"},"trusted":true},"outputs":[],"source":["import cv2\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{},"source":["# Setup\n","Pip install ultralytics and dependencies and check software and hardware."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-29T16:03:20.678488Z","iopub.status.busy":"2023-09-29T16:03:20.678161Z","iopub.status.idle":"2023-09-29T16:03:35.731979Z","shell.execute_reply":"2023-09-29T16:03:35.730941Z","shell.execute_reply.started":"2023-09-29T16:03:20.678459Z"},"trusted":true},"outputs":[],"source":["%pip install ultralytics\n","import ultralytics\n","from ultralytics import YOLO\n","ultralytics.checks()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-17T18:32:50.154220Z","iopub.status.busy":"2023-09-17T18:32:50.153521Z","iopub.status.idle":"2023-09-17T18:33:07.481571Z","shell.execute_reply":"2023-09-17T18:33:07.480264Z","shell.execute_reply.started":"2023-09-17T18:32:50.154183Z"},"trusted":true},"outputs":[],"source":["pip install --force-reinstall numpy==1.23.0"]},{"cell_type":"markdown","metadata":{},"source":["# Train\n","From dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!yolo train model=\"yolov8n.pt\" data=\"/home/ubuntu/technoserve/new_dset/data.yaml\" epochs=400 imgsz=1024 single_cls=True batch=64\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-28T21:57:48.569757Z","iopub.status.busy":"2023-09-28T21:57:48.569041Z"},"trusted":true},"outputs":[],"source":["!yolo train model=\"yolov8n.pt\" data=\"/home/ubuntu/technoserve/new_dset/data.yaml\" epochs=100 imgsz=512 single_cls=True"]},{"cell_type":"markdown","metadata":{},"source":["# Test the model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from IPython import display\n","display.display(display.Image(\"/mnt/rd/labeled/overripe/msg390418808-144197.jpg\"))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = YOLO('runs/detect/train21/weights/best.pt')  # load a custom model\n","model.conf = 0.03"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# save as torchscript\n","model.export(format=\"torchscript\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from PIL import Image\n","import torch\n","import torchvision.transforms as T\n","from torchvision import models\n","from torch import nn\n","# we need this function as it's being called (it's in the same file)\n","def xywh2xyxy(x):\n","    \"\"\"\n","    Convert bounding box coordinates from (x, y, width, height) format to (x1, y1, x2, y2) format where (x1, y1) is the\n","    top-left corner and (x2, y2) is the bottom-right corner.\n","\n","    Args:\n","        x (np.ndarray) or (torch.Tensor): The input bounding box coordinates in (x, y, width, height) format.\n","    Returns:\n","        y (np.ndarray) or (torch.Tensor): The bounding box coordinates in (x1, y1, x2, y2) format.\n","    \"\"\"\n","    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n","    y[..., 0] = x[..., 0] - x[..., 2] / 2  # top left x\n","    y[..., 1] = x[..., 1] - x[..., 3] / 2  # top left y\n","    y[..., 2] = x[..., 0] + x[..., 2] / 2  # bottom right x\n","    y[..., 3] = x[..., 1] + x[..., 3] / 2  # bottom right y\n","    return y\n","\n","class WrapperModel2(nn.Module):\n","  def __init__(self, model: torch.jit._script.RecursiveScriptModule):\n","    super().__init__()\n","    # model is the YOLO exported torchscript model\n","    self.model = model\n","  \n","  def forward(self, input_tensor: torch.Tensor, conf_thres:float=0.25):    \n","    multi_label=False\n","    max_time_img=0.05\n","    multi_label=False\n","    labels=()\n","    classes=None\n","    max_nms=30000\n","    max_wh=7680\n","    agnostic=False\n","    # conf_thres=0\n","    \n","    prediction = self.model(input_tensor)\n","    \n","    # Copy from /ultralytics/yolo/utils/ops.py non_max_suppression\n","    bs = prediction.shape[0]  # batch size\n","    nc = (prediction.shape[1] - 4)  # number of classes\n","    nm = prediction.shape[1] - nc - 4\n","    mi = 4 + nc  # mask start index\n","    xc = prediction[:, 4:mi].amax(1) > conf_thres\n","    \n","    # Settings\n","    # min_wh = 2  # (pixels) minimum box width and height\n","    \n","    time_limit = 0.5 + max_time_img * bs  # seconds to quit after\n","    redundant = True  # require redundant detections\n","    multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n","    merge = False  # use merge-NMS\n","    \n","    # t = time.time()\n","    output = [torch.zeros((0, 6 + nm), device=prediction.device)] * bs\n","    for xi, x in enumerate(prediction):  # image index, image inference\n","        print(f\"prediction: {xi}\")\n","        # Apply constraints\n","        # x[((x[:, 2:4] < min_wh) | (x[:, 2:4] > max_wh)).any(1), 4] = 0  # width-height\n","        x = x.transpose(0, -1)[xc[xi]]  # confidence\n","\n","        # Cat apriori labels if autolabelling\n","        # if labels and len(labels[xi]):\n","        #     lb = labels[xi]\n","        #     v = torch.zeros((len(lb), nc + nm + 5), device=x.device)\n","        #     v[:, :4] = lb[:, 1:5]  # box\n","        #     v[range(len(lb)), lb[:, 0].long() + 4] = 1.0  # cls\n","        #     x = torch.cat((x, v), 0)\n","\n","        # If none remain process next image\n","        print(x.shape)\n","        if not x.shape[0]:\n","            continue\n","\n","        # Detections matrix nx6 (xyxy, conf, cls)\n","        box, cls, mask = x.split((4, nc, nm), 1)\n","        box = xywh2xyxy(box)  # center_x, center_y, width, height) to (x1, y1, x2, y2)\n","        # if multi_label:\n","        #     i, j = (cls > conf_thres).nonzero(as_tuple=False).T\n","        #     x = torch.cat((box[i], x[i, 4 + j, None], j[:, None].float(), mask[i]), 1)\n","        # else:  # best class only\n","        #     conf, j = cls.max(1, keepdim=True)\n","        #     x = torch.cat((box, conf, j.float(), mask), 1)[conf.view(-1) > conf_thres]\n","            \n","        # * This is the \"else\" condition above\n","        conf, j = cls.max(1, keepdim=True)\n","        x = torch.cat((box, conf, j.float(), mask), 1)[conf.view(-1) > conf_thres]\n","\n","        # Filter by class\n","        if classes is not None:\n","            x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n","\n","        # Apply finite constraint\n","        # if not torch.isfinite(x).all():\n","        #     x = x[torch.isfinite(x).all(1)]\n","\n","        # Check shape\n","        n = x.shape[0]  # number of boxes\n","        if not n:  # no boxes\n","            continue\n","        x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence and remove excess boxes\n","\n","        # Batched NMS\n","        c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n","        boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n","\n","        # forcefully return on the first prediction (which is OK for inference)\n","        return x , boxes, scores # actually only need x, while the others could be transformed in Android side from x"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# SAVE TORCH SCR"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["torch_script_model = torch.jit.load(\"/home/ubuntu/technoserve/runs/detect/train21/weights/best.torchscript\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torchvision\n","w_model_2 = WrapperModel2(torch_script_model) # or in attempt 5, using WrapperModel(model)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torch.utils.mobile_optimizer import optimize_for_mobile\n","scripted_wrapped_model2 = torch.jit.script(w_model_2)\n","optimized_torchscript_b4_nms_model = optimize_for_mobile(scripted_wrapped_model2)\n","optimized_torchscript_b4_nms_model._save_for_lite_interpreter(\"./mobile_model_b4_nms.ptl\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["rslt = model(\"/mnt/rd/labeled/overripe/msg390418808-144197.jpg\")#,predictor=ultralytics.engine.predictor.BasePredictor)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["rslt[0].boxes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Load the image\n","image = cv2.imread(\"/mnt/rd/labeled/overripe/msg5421324117-146123.jpg\")\n","\n","# Define the number of patches in each dimension\n","n_patches_h = 1  # number of patches vertically\n","n_patches_w = 1  # number of patches horizontally\n","\n","# Calculate patch sizes\n","h, w, _ = image.shape\n","patch_h = h // n_patches_h\n","patch_w = w // n_patches_w\n","\n","# Create patches\n","patches = []\n","for i in range(n_patches_h):\n","    for j in range(n_patches_w):\n","        patch = image[i*patch_h:(i+1)*patch_h, j*patch_w:(j+1)*patch_w]\n","        patches.append(patch)\n","\n","# Detect objects in each patch\n","results = []\n","for patch in patches:\n","    results.append(model(patch)[0].plot(labels=False))\n","\n","# Merge the results\n","merged_result = np.zeros((h, w, 3), dtype=np.uint8)\n","for i in range(n_patches_h):\n","    for j in range(n_patches_w):\n","        merged_result[i*patch_h:(i+1)*patch_h, j*patch_w:(j+1)*patch_w] = results[i*n_patches_w+j]\n","\n","# Display the result\n","plt.figure(figsize=(6, 6))\n","plt.imshow((cv2.cvtColor(merged_result, cv2.COLOR_BGR2RGB)))\n","# plt.axis('off')\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Detect objects in each patch\n","results = []\n","detected_objects = []\n","for patch in patches:\n","    result = model(patch)[0]\n","    results.append(result.plot(labels=False))\n","    \n","    # Extract bounding boxes and crop objects\n","    for box in result.boxes.xyxy:\n","        x1, y1, x2, y2 = box.tolist()\n","        object_crop = patch[int(y1):int(y2), int(x1):int(x2)]\n","        detected_objects.append(object_crop)\n","\n","# Function to resize and compute features\n","def compute_features(image, target_size=(64, 64)):\n","    # Resize the image\n","    resized = cv2.resize(image, target_size)\n","    \n","    # Compute color histogram\n","    hist = cv2.calcHist([resized], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n","    hist_feature = hist.flatten() / hist.sum()  # Normalize the histogram\n","    \n","    # Compute shape features\n","    gray = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)\n","    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n","    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","    if contours:\n","        cnt = max(contours, key=cv2.contourArea)\n","        area = cv2.contourArea(cnt) / (target_size[0] * target_size[1])  # Normalized area\n","        perimeter = cv2.arcLength(cnt, True) / (2 * (target_size[0] + target_size[1]))  # Normalized perimeter\n","        shape_feature = [area, perimeter]\n","    else:\n","        shape_feature = [0, 0]\n","    \n","    # Combine features\n","    return np.concatenate([hist_feature, shape_feature])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Compute features for all detected objects\n","features = [compute_features(obj) for obj in detected_objects]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.cluster import KMeans"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Perform K-means clustering\n","n_clusters = 4  # You can adjust this number\n","kmeans = KMeans(n_clusters=n_clusters)\n","labels = kmeans.fit_predict(features)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Assign colors to clusters\n","colors = [(255,0,0), (0,255,0), (0,0,255), (255, 255, 0)]  # RGB colors for each cluster\n","\n","# Merge the results with cluster colors\n","merged_result = np.zeros((h, w, 3), dtype=np.uint8)\n","label_index = 0\n","for i, patch_result in enumerate(results):\n","    patch_with_labels = patch_result.copy()\n","    for box in model(patches[i])[0].boxes.xyxy:\n","        x1, y1, x2, y2 = box.tolist()\n","        color = colors[labels[label_index]]\n","        cv2.rectangle(patch_with_labels, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n","        label_index += 1\n","    \n","    i, j = divmod(i, n_patches_w)\n","    merged_result[i*patch_h:(i+1)*patch_h, j*patch_w:(j+1)*patch_w] = patch_with_labels\n","\n","# Display the result\n","plt.figure(figsize=(12, 12))\n","plt.imshow(cv2.cvtColor(merged_result, cv2.COLOR_BGR2RGB))\n","plt.axis('off')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-29T16:06:40.738997Z","iopub.status.busy":"2023-09-29T16:06:40.738401Z","iopub.status.idle":"2023-09-29T16:06:50.757036Z","shell.execute_reply":"2023-09-29T16:06:50.756218Z","shell.execute_reply.started":"2023-09-29T16:06:40.738967Z"},"trusted":true},"outputs":[],"source":["imageUrl = '/mnt/rd/labeled/overripe/msg390418808-144197.jpg'\n","\n","# Load a model\n","#model = YOLO('yolov8n-seg.pt')  # load an official model\n","\n","# Predict with the model\n","results = model(imageUrl)  # predict on an image\n","\n","results_image = results[0].plot(labels=False)\n","plt.imshow(cv2.cvtColor(results_image, cv2.COLOR_BGR2RGB))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","from transformers import GLPNImageProcessor, GLPNForDepthEstimation\n","import torch\n","import numpy as np\n","from PIL import Image\n","import requests\n","\n","image = Image.open(\"/mnt/rd/labeled/overripe/msg390418808-144140.jpg\")\n","\n","processor = GLPNImageProcessor.from_pretrained(\"vinvino02/glpn-kitti\")\n","model = GLPNForDepthEstimation.from_pretrained(\"vinvino02/glpn-kitti\")\n","\n","# prepare image for the model\n","inputs = processor(images=image, return_tensors=\"pt\")\n","\n","with torch.no_grad():\n","    outputs = model(**inputs)\n","    predicted_depth = outputs.predicted_depth\n","\n","# interpolate to original size\n","prediction = torch.nn.functional.interpolate(\n","    predicted_depth.unsqueeze(1),\n","    size=image.size[::-1],\n","    mode=\"bicubic\",\n","    align_corners=False,\n",")\n","\n","# visualize the prediction\n","output = prediction.squeeze().cpu().numpy()\n","formatted = (output * 255 / np.max(output)).astype(\"uint8\")\n","depth = Image.fromarray(formatted)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["depth"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-29T16:11:53.966627Z","iopub.status.busy":"2023-09-29T16:11:53.966271Z","iopub.status.idle":"2023-09-29T16:12:14.146165Z","shell.execute_reply":"2023-09-29T16:12:14.145248Z","shell.execute_reply.started":"2023-09-29T16:11:53.966599Z"},"trusted":true},"outputs":[],"source":["import cv2\n","import numpy as np\n","from ultralytics import YOLO\n","from scipy.cluster.hierarchy import linkage, fcluster\n","from matplotlib import pyplot as plt\n","def apply_clahe(img):\n","    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n","    l, a, b = cv2.split(lab)\n","    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n","    cl = clahe.apply(l)\n","    limg = cv2.merge((cl,a,b))\n","    return cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n","\n","def adjust_gamma(img, gamma=1.0):\n","    inv_gamma = 1.0 / gamma\n","    table = np.array([((i / 255.0) ** inv_gamma) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n","    return cv2.LUT(img, table)\n","\n","def non_max_suppression_fast(boxes, scores, overlapThresh):\n","    if len(boxes) == 0:\n","        return []\n","\n","    if boxes.dtype.kind == \"i\":\n","        boxes = boxes.astype(\"float\")\n","\n","    pick = []\n","    x1, y1, x2, y2 = boxes[:,0], boxes[:,1], boxes[:,2], boxes[:,3]\n","    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n","    idxs = np.argsort(scores)\n","\n","    while len(idxs) > 0:\n","        last = len(idxs) - 1\n","        i = idxs[last]\n","        pick.append(i)\n","\n","        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n","        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n","        xx2 = np.minimum(x2[i], x2[idxs[:last]])\n","        yy2 = np.minimum(y2[i], y2[idxs[:last]])\n","\n","        w = np.maximum(0, xx2 - xx1 + 1)\n","        h = np.maximum(0, yy2 - yy1 + 1)\n","\n","        overlap = (w * h) / area[idxs[:last]]\n","        idxs = np.delete(idxs, np.concatenate(([last], np.where(overlap > overlapThresh)[0])))\n","\n","    return pick\n","\n","def efficient_multi_scale_detection(image_path, model_path, conf_threshold=0.1, iou_threshold=0.3):\n","    model = YOLO(model_path)\n","    original_image = cv2.imread(image_path)\n","    h, w = original_image.shape[:2]\n","\n","    scale_factors = [1.0, 1.25]\n","    patch_sizes = [(h//1, w//1), (h//2, w//2)]\n","\n","    all_detections = []\n","\n","    for scale in scale_factors:\n","        scaled_image = cv2.resize(original_image, (int(w*scale), int(h*scale)))\n","        \n","        enhanced_images = [\n","            scaled_image,\n","            apply_clahe(scaled_image),\n","            adjust_gamma(scaled_image, 0.8),\n","            adjust_gamma(scaled_image, 1.2)\n","        ]\n","\n","        for img in enhanced_images:\n","            results = model(img, conf=conf_threshold)\n","            for r in results:\n","                boxes = r.boxes.xyxy.cpu().numpy()\n","                scores = r.boxes.conf.cpu().numpy()\n","                for box, score in zip(boxes, scores):\n","                    all_detections.append({\n","                        'box': box / scale,\n","                        'score': score,\n","                        'scale': scale\n","                    })\n","\n","            for patch_h, patch_w in patch_sizes:\n","                for i in range(0, img.shape[0] - patch_h + 1, patch_h // 2):\n","                    for j in range(0, img.shape[1] - patch_w + 1, patch_w // 2):\n","                        patch = img[i:i+patch_h, j:j+patch_w]\n","                        results = model(patch, conf=conf_threshold)\n","                        for r in results:\n","                            boxes = r.boxes.xyxy.cpu().numpy()\n","                            scores = r.boxes.conf.cpu().numpy()\n","                            for box, score in zip(boxes, scores):\n","                                adjusted_box = [\n","                                    (box[0] + j) / scale,\n","                                    (box[1] + i) / scale,\n","                                    (box[2] + j) / scale,\n","                                    (box[3] + i) / scale\n","                                ]\n","                                all_detections.append({\n","                                    'box': adjusted_box,\n","                                    'score': score,\n","                                    'scale': scale\n","                                })\n","\n","    boxes = np.array([d['box'] for d in all_detections])\n","    scores = np.array([d['score'] for d in all_detections])\n","\n","    keep = non_max_suppression_fast(boxes, scores, iou_threshold)\n","    filtered_detections = [all_detections[i] for i in keep]\n","\n","    detection_features = np.array([[d['box'][0], d['box'][1], d['box'][2], d['box'][3], d['score'], d['scale']] for d in filtered_detections])\n","    Z = linkage(detection_features, 'ward')\n","    clusters = fcluster(Z, t=0.5, criterion='distance')\n","\n","    final_results = []\n","    for cluster_id in np.unique(clusters):\n","        cluster_detections = [filtered_detections[i] for i in range(len(filtered_detections)) if clusters[i] == cluster_id]\n","        best_detection = max(cluster_detections, key=lambda x: x['score'])\n","        final_results.append(best_detection)\n","\n","    final_results.sort(key=lambda x: x['score'], reverse=True)\n","\n","    return final_results, original_image\n","\n","def merge_close_boundaries(detections, iou_threshold=0.7):\n","    merged = []\n","    detections.sort(key=lambda x: x['score'], reverse=True)\n","    \n","    for det in detections:\n","        if not merged:\n","            merged.append(det)\n","        else:\n","            should_merge = False\n","            for i, m_det in enumerate(merged):\n","                iou = calculate_iou(det['box'], m_det['box'])\n","                if iou > iou_threshold:\n","                    merged[i]['box'] = [\n","                        min(det['box'][0], m_det['box'][0]),\n","                        min(det['box'][1], m_det['box'][1]),\n","                        max(det['box'][2], m_det['box'][2]),\n","                        max(det['box'][3], m_det['box'][3])\n","                    ]\n","                    merged[i]['score'] = max(det['score'], m_det['score'])\n","                    should_merge = True\n","                    break\n","            if not should_merge:\n","                merged.append(det)\n","    \n","    return merged\n","\n","def calculate_iou(box1, box2):\n","    x1 = max(box1[0], box2[0])\n","    y1 = max(box1[1], box2[1])\n","    x2 = min(box1[2], box2[2])\n","    y2 = min(box1[3], box2[3])\n","    \n","    intersection = max(0, x2 - x1) * max(0, y2 - y1)\n","    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n","    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n","    \n","    iou = intersection / float(area1 + area2 - intersection)\n","    return iou\n","\n","\n","\n","# Usage\n","# if __name__ == '__main__':\n","image_path = \"/mnt/rd/labeled/overripe/msg390418808-144197.jpg\"\n","model_path = 'runs/detect/train17/weights/best.pt'\n","detections, original_image = efficient_multi_scale_detection(image_path, model_path)\n","\n","# Merge close boundaries\n","detections = merge_close_boundaries(detections)\n","\n","# Visualize results\n","result_image = original_image.copy()\n","for det in detections:\n","    box = det['box']\n","    score = det['score']\n","    cv2.rectangle(result_image, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (0, 255, 0), 2)\n","    # cv2.putText(result_image, f\"{score:.2f}\", (int(box[0]), int(box[1])-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,255,0), 2)\n","\n","# plot the image\n","plt.figure(figsize=(12, 12))\n","plt.imshow(cv2.cvtColor(result_image, cv2.COLOR_BGR2RGB))\n","plt.axis('off')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import cv2\n","import numpy as np\n","from ultralytics import YOLO\n","import time\n","\n","def optimized_multi_scale_detection(image_path, model_path, conf_threshold=0.1, iou_threshold=0.5):\n","    def apply_clahe(img):\n","        lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n","        l, a, b = cv2.split(lab)\n","        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n","        cl = clahe.apply(l)\n","        limg = cv2.merge((cl,a,b))\n","        return cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n","\n","    def non_max_suppression_fast(boxes, scores, overlapThresh):\n","        if len(boxes) == 0:\n","            return []\n","\n","        if boxes.dtype.kind == \"i\":\n","            boxes = boxes.astype(\"float\")\n","\n","        pick = []\n","\n","        x1 = boxes[:, 0]\n","        y1 = boxes[:, 1]\n","        x2 = boxes[:, 2]\n","        y2 = boxes[:, 3]\n","\n","        area = (x2 - x1 + 1) * (y2 - y1 + 1)\n","        idxs = np.argsort(scores)\n","\n","        while len(idxs) > 0:\n","            last = len(idxs) - 1\n","            i = idxs[last]\n","            pick.append(i)\n","\n","            xx1 = np.maximum(x1[i], x1[idxs[:last]])\n","            yy1 = np.maximum(y1[i], y1[idxs[:last]])\n","            xx2 = np.minimum(x2[i], x2[idxs[:last]])\n","            yy2 = np.minimum(y2[i], y2[idxs[:last]])\n","\n","            w = np.maximum(0, xx2 - xx1 + 1)\n","            h = np.maximum(0, yy2 - yy1 + 1)\n","\n","            overlap = (w * h) / area[idxs[:last]]\n","\n","            idxs = np.delete(idxs, np.concatenate(([last],\n","                np.where(overlap > overlapThresh)[0])))\n","\n","        return pick\n","\n","    # Load the model\n","    model = YOLO(model_path)\n","\n","    # Load and preprocess the image\n","    original_image = cv2.imread(image_path)\n","    h, w = original_image.shape[:2]\n","\n","    # Define scale factors (reduced number of scales)\n","    scale_factors = [0.75, 1.0, 1.25]\n","\n","    all_detections = []\n","\n","    # Multi-scale detection\n","    for scale in scale_factors:\n","        scaled_image = cv2.resize(original_image, (int(w*scale), int(h*scale)))\n","        \n","        # Apply image enhancement\n","        enhanced_image = apply_clahe(scaled_image)\n","\n","        # Whole image detection\n","        results = model(enhanced_image, conf=conf_threshold)\n","        for r in results:\n","            boxes = r.boxes.xyxy.cpu().numpy()\n","            scores = r.boxes.conf.cpu().numpy()\n","            for box, score in zip(boxes, scores):\n","                all_detections.append({\n","                    'box': box / scale,\n","                    'score': score,\n","                    'scale': scale\n","                })\n","\n","        # Adaptive patch-based detection\n","        if scale != 1.0:  # Only do patch-based detection for non-original scales\n","            patch_size = (h//3, w//3)\n","            stride = patch_size[0] // 2  # 50% overlap\n","\n","            for i in range(0, scaled_image.shape[0] - patch_size[0] + 1, stride):\n","                for j in range(0, scaled_image.shape[1] - patch_size[1] + 1, stride):\n","                    patch = enhanced_image[i:i+patch_size[0], j:j+patch_size[1]]\n","                    results = model(patch, conf=conf_threshold)\n","                    for r in results:\n","                        boxes = r.boxes.xyxy.cpu().numpy()\n","                        scores = r.boxes.conf.cpu().numpy()\n","                        for box, score in zip(boxes, scores):\n","                            # Adjust coordinates to original image space\n","                            adjusted_box = [\n","                                (box[0] + j) / scale,\n","                                (box[1] + i) / scale,\n","                                (box[2] + j) / scale,\n","                                (box[3] + i) / scale\n","                            ]\n","                            all_detections.append({\n","                                'box': adjusted_box,\n","                                'score': score,\n","                                'scale': scale\n","                            })\n","\n","    # Prepare data for non-max suppression\n","    boxes = np.array([d['box'] for d in all_detections])\n","    scores = np.array([d['score'] for d in all_detections])\n","\n","    # Perform fast non-max suppression\n","    keep = non_max_suppression_fast(boxes, scores, iou_threshold)\n","\n","    # Filter detections\n","    filtered_detections = [all_detections[i] for i in keep]\n","\n","    # Sort final results by confidence score\n","    filtered_detections.sort(key=lambda x: x['score'], reverse=True)\n","\n","    return filtered_detections, original_image\n","\n","# Usage\n","start_time = time.time()\n","\n","image_path = \"/mnt/rd/labeled/ripe/msg6158697711-145910.jpg\"\n","model_path = 'runs/detect/train17/weights/best.pt'\n","detections, original_image = optimized_multi_scale_detection(image_path, model_path)\n","\n","end_time = time.time()\n","print(f\"Detection time: {end_time - start_time:.2f} seconds\")\n","\n","# Visualize results\n","result_image = original_image.copy()\n","for det in detections:\n","    box = det['box']\n","    score = det['score']\n","    ccolor = (0, 255, 0) if score > 0.5 else ((255, 0, 0) if score < 0.2 else (0, 0, 255))\n","    cv2.rectangle(result_image, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), ccolor, 2)\n","    # cv2.putText(result_image, f\"{score:.2f}\", (int(box[0]), int(box[1])-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n","\n","plt.figure(figsize=(12, 12))\n","plt.imshow(cv2.cvtColor(result_image, cv2.COLOR_BGR2RGB))\n","plt.axis('off')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import cv2\n","import numpy as np\n","\n","def generate_synthetic_image(size=(128, 128)):\n","    image = np.zeros((size[0], size[1], 3), dtype=np.uint8)\n","    \n","    # Choose a random background type\n","    bg_type = np.random.choice(['solid', 'gradient', 'noise'])\n","    \n","    if bg_type == 'solid':\n","        # Solid color background\n","        color = np.random.randint(0, 256, 3)\n","        image[:] = color\n","    \n","    elif bg_type == 'gradient':\n","        # Gradient background\n","        start_color = np.random.randint(0, 256, 3)\n","        end_color = np.random.randint(0, 256, 3)\n","        for i in range(size[0]):\n","            ratio = i / size[0]\n","            color = start_color * (1 - ratio) + end_color * ratio\n","            image[i, :] = color\n","    \n","    else:  # noise\n","        # Colored noise background\n","        image = np.random.randint(0, 256, size=(size[0], size[1], 3), dtype=np.uint8)\n","    \n","    # Add some shapes\n","    num_shapes = np.random.randint(0, 6)\n","    for _ in range(num_shapes):\n","        shape_type = np.random.choice(['circle', 'rectangle', 'line'])\n","        color = np.random.randint(0, 256, 3).tolist()\n","        \n","        if shape_type == 'circle':\n","            center = tuple(np.random.randint(0, size[0], 2))\n","            radius = np.random.randint(10, 50)\n","            cv2.circle(image, center, radius, color, -1)\n","        \n","        elif shape_type == 'rectangle':\n","            pt1 = tuple(np.random.randint(0, size[0], 2))\n","            pt2 = tuple(np.random.randint(0, size[0], 2))\n","            cv2.rectangle(image, pt1, pt2, color, -1)\n","        \n","        else:  # line\n","            pt1 = tuple(np.random.randint(0, size[0], 2))\n","            pt2 = tuple(np.random.randint(0, size[0], 2))\n","            thickness = np.random.randint(1, 10)\n","            cv2.line(image, pt1, pt2, color, thickness)\n","    \n","    # Apply some blur to make it look more natural\n","    blur_amount = np.random.randint(0, 3) * 2 + 1\n","    image = cv2.GaussianBlur(image, (blur_amount, blur_amount), 0)\n","    \n","    # Randomly adjust brightness\n","    brightness = np.random.uniform(0.5, 1.5)\n","    image = np.clip(image * brightness, 0, 255).astype(np.uint8)\n","    \n","    return image\n","\n","# You can test the function like this:\n","# test_image = generate_synthetic_image()\n","# cv2.imshow('Synthetic Image', test_image)\n","# cv2.waitKey(0)\n","# cv2.destroyAllWindows()\n","\n","import glob\n","import torch\n","import cv2\n","import random\n","import numpy as np\n","def yolo_to_x1y1x2y2(x_center, y_center, width, height, img_width, img_height):\n","    x_center *= img_width\n","    y_center *= img_height\n","    width *= img_width\n","    height *= img_height\n","    \n","    x1 = int(x_center - width / 2)\n","    y1 = int(y_center - height / 2)\n","    x2 = int(x_center + width / 2)\n","    y2 = int(y_center + height / 2)\n","    \n","    return x1, y1, x2, y2\n","def pre_process_image(image, noise_up=False):\n","    if noise_up:\n","        # create noise\n","        if random.random() < 0.5:\n","            noise = np.random.normal(0, 255, image.shape) * 0.05\n","            image = image + noise\n","            image = np.clip(image, 0, 255)\n","            image = image.astype(np.uint8)\n","        else:\n","\n","            # add brightness\n","            image = image + ((((random.random()*2)-1) * 0.05)*255)\n","            image = np.clip(image, 0, 255)\n","            # convert to int\n","            image = image.astype(np.uint8)\n","\n","        if random.random() < 0.4:\n","            # split the image into half , \n","            devisor_component = (random.random())+1\n","            if random.random() < 0.5:\n","                vertical = False\n","                if vertical:\n","                    middle_half = int(image.shape[0] / devisor_component)\n","                    image = image[:middle_half, :]\n","                else:\n","                    middle_half = int(image.shape[1] / devisor_component)\n","                    image = image[:, :middle_half]\n","            else:\n","                # split the image into half , \n","                vertical = False\n","                if vertical:\n","                    middle_half = int(image.shape[0] / devisor_component)\n","                    image = image[middle_half:, :]\n","                else:\n","                    middle_half = int(image.shape[1] / devisor_component)\n","                    image = image[:, middle_half:]\n","\n","\n","\n","    image = cv2.resize(image, (64, 64))\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","    \n","    image = image / 255.0\n","    image = np.transpose(image, (2, 0, 1))\n","    return image\n","class TrainingDataset(torch.utils.data.Dataset):\n","    def __init__(self, selection=\"train\", data_loader_repeat=1):\n","        image_paths = glob.glob(\"/home/ubuntu/technoserve/neupw.v1i.yolov5pytorch/train/images/*.jpg\")\n","        label_stores = glob.glob(\"/home/ubuntu/technoserve/neupw.v1i.yolov5pytorch/train/labels/*.txt\")\n","\n","        self.image_labels = {}\n","\n","        for image_path in image_paths:\n","            image_id = image_path.split('/')[-1].split('.')[0]\n","            self.image_labels[image_path] = []\n","            # find from label stores\n","            for label_store in label_stores:\n","                label_id = label_store.split('/')[-1].split('.')[0]\n","                if label_id == image_id:\n","                    with open(label_store, 'r') as f:\n","                        for line in f:\n","                            classitem = line.strip().split(' ')\n","                            class_idx = int(classitem[0])\n","                            x_cent = float(classitem[1])\n","                            y_cent = float(classitem[2])\n","                            i_wid = float(classitem[3])\n","                            i_hei = float(classitem[4])\n","                            self.image_labels[image_path].append([class_idx, (x_cent, y_cent, i_wid, i_hei)])\n","\n","        self.precompute_bounding_boxes()\n","        _class_image_pairs = []\n","\n","        for image_path, labels in self.image_labels.items():\n","            # load image \n","            image = cv2.imread(image_path)\n","            for label in labels:\n","                class_idx = label[0]\n","                # cut the image\n","                x_cent, y_cent, i_wid, y_wid = label[1]\n","                x1, y1, x2, y2 = yolo_to_x1y1x2y2(x_cent, y_cent, i_wid, y_wid, image.shape[1], image.shape[0])\n","                # print(x1, y1, x2, y2)\n","                crop_image = image[int(y1):int(y2), int(x1):int(x2)]\n","               \n","                \n","                _class_image_pairs.append((class_idx, crop_image))\n","\n","            _negative_samples = self.find_negative_samples(image, image_path)\n","            self.negative_samples = _negative_samples\n","            print(\"added negative samples: \", len(_negative_samples))\n","            _class_image_pairs.extend(_negative_samples)\n","    \n","            \n","        self.data = _class_image_pairs\n","\n","        # split into train and test\n","        train_size = int(0.85 * len(self.data))\n","        test_size = len(self.data) - train_size\n","        self.train_data, self.test_data = torch.utils.data.random_split(self.data, [train_size, test_size])\n","        self.noise_up  = data_loader_repeat > 1\n","        if selection == \"train\":\n","            self.data = list(self.train_data) * data_loader_repeat\n","        else:\n","            self.data =list( self.test_data)\n","\n","    def precompute_bounding_boxes(self):\n","        self.bounding_boxes = {}\n","        for image_path, labels in self.image_labels.items():\n","            image = cv2.imread(image_path)\n","            height, width = image.shape[:2]\n","            boxes = []\n","            for label in labels:\n","                _, (x_cent, y_cent, i_wid, i_hei) = label\n","                bx1, by1, bx2, by2 = yolo_to_x1y1x2y2(x_cent, y_cent, i_wid, i_hei, width, height)\n","                boxes.append([bx1, by1, bx2, by2])\n","            self.bounding_boxes[image_path] = np.array(boxes)\n","\n","    def find_negative_samples(self, image, image_path):\n","        negative_samples = []\n","        height, width = image.shape[:2]\n","        \n","        \n","        boxes = self.bounding_boxes[image_path]\n","        \n","        for _ in range(128):\n","            attempts = 0\n","            while attempts < 98:  # Limit attempts to avoid infinite loop\n","                crop_size = np.random.randint(32, 64)\n","                crop_ratio = np.random.uniform(0.7, 1.3)\n","                x1 = np.random.randint(0, width - (crop_size))\n","                y1 = np.random.randint(0, height - int(crop_size * (1 / crop_ratio)))\n","                x2, y2 = x1 + crop_size, y1 + int(crop_size * (1 / crop_ratio))\n","                \n","                # Vectorized overlap checking\n","                overlap = np.logical_and.reduce((\n","                    x1 < boxes[:, 2],\n","                    x2 > boxes[:, 0],\n","                    y1 < boxes[:, 3],\n","                    y2 > boxes[:, 1]\n","                ))\n","                \n","                if not np.any(overlap):\n","                    crop_image = image[y1:y2, x1:x2]\n","                    negative_samples.append((3, crop_image))\n","                    break\n","                \n","                attempts += 1\n","        \n","        return negative_samples\n","\n","    def __len__(self):\n","        return int(len(self.data)*1.1)\n","\n","    def __getitem__(self, idx):\n","        # if idx >= len(self.data):, then return synthetic image\n","        if idx >= len(self.data):\n","            data_itm = generate_synthetic_image()\n","            return pre_process_image(data_itm[1], self.noise_up), 3\n","        \n","        data_itm= (self.data)[idx]\n","        # print(data_itm[1].shape)\n","        return pre_process_image(data_itm[1], self.noise_up), data_itm[0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_dataset = TrainingDataset(\"train\", data_loader_repeat=10)\n","test_dataset = TrainingDataset(\"test\")\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# from matplotlib import pyplot as plt\n","# # train_dataset.negative_samples[0][1].shape\n","# plt.imshow(cv2.cvtColor(train_dataset.negative_samples[99][1], cv2.COLOR_BGR2RGB))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class ClassifierModel(nn.Module):\n","    def __init__(self):\n","        super(ClassifierModel, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(32)\n","\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(64)\n","\n","        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n","        self.bn3 = nn.BatchNorm2d(128)\n","\n","        \n","        self.dense_linears= nn.Sequential(\n","            nn.Linear(128 * 8 * 8, 128),\n","            nn.ReLU(),\n","            nn.Dropout(0.1),\n","            nn.Linear(128, 64),\n","            nn.ReLU(),\n","            nn.Dropout(0.1),\n","            nn.Linear(64, 4)\n","\n","        )\n","        \n","\n","        self.dropout = nn.Dropout(0.1)\n","\n","        # self.init_weights()\n","\n","    def init_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n","                nn.init.kaiming_normal_(m.weight)\n","                if m.bias is not None:\n","                    nn.init.zeros_(m.bias)\n","\n","    def forward(self, x):\n","        # x shape: (B, 3, 128, 128)\n","        x = F.relu(self.bn1(self.conv1(x)))\n","        x = F.max_pool2d(x, 2)  # 64x64\n","        x = self.dropout(x)\n","        \n","        x = F.relu(self.bn2(self.conv2(x)))\n","        x = F.max_pool2d(x, 2)  # 32x32\n","        x = self.dropout(x)\n","\n","        x = F.relu(self.bn3(self.conv3(x)))\n","        x = F.max_pool2d(x, 2)  # 32x32\n","        x = self.dropout(x)\n","\n","        # x = F.relu(self.bn3(self.conv3(x)))\n","        # x = F.max_pool2d(x, 2)  # 32x32\n","        # x = self.dropout(x)\n","\n","        # print (\"x shape\", x.shape)\n","        x = x.view(x.shape[0], -1)\n","        x = self.dense_linears(x)\n","        \n","        return x  # shape: (B, 3)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["init_model = ClassifierModel().cuda()\n","optimizer = torch.optim.Adam(init_model.parameters(), lr=0.0001)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# torch.save(init_model.state_dict(), \"/home/ubuntu/technoserve/classifier.pth\")\n","# init_model.load_state_dict(torch.load(\"/home/ubuntu/technoserve/classifier.pth\"))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# torch script the model\n","init_model.eval()\n","from torch.utils.mobile_optimizer import optimize_for_mobile\n","\n","scripted_module = torch.jit.script(init_model.cpu())\n","optimized_scripted_module = optimize_for_mobile(scripted_module)\n","optimized_scripted_module.save(\"./classifier.pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","from collections import defaultdict\n","\n","# Define class weights\n","class_weights = torch.tensor([1.0, 1.0, 1.0, 0.4]).cuda()  # Less weight for \"not-bean\" class\n","major_penalty = 5.0  # Penalty multiplier for misclassifying beans as not-beans\n","\n","for epoch in range(40):\n","    init_model.train()\n","    for i, (crop_image, class_idx) in enumerate(train_loader):\n","        crop_image = crop_image.detach().cuda().float()\n","        class_idx = class_idx.cuda()\n","        optimizer.zero_grad()\n","        output = init_model(crop_image)\n","\n","        # print(output.shape, class_idx.shape, output.min(), output.max())\n","        \n","        # Calculate standard cross-entropy loss\n","        loss = F.cross_entropy(output, class_idx, weight=class_weights)\n","        \n","        # Add major penalty for misclassifying beans as not-beans\n","        _, predicted = torch.max(output, 1)\n","        bean_mask = class_idx != 3  # True for all bean classes\n","        misclassified_as_not_bean = (predicted == 3) & bean_mask\n","        penalty = major_penalty * misclassified_as_not_bean.float().mean()\n","        \n","        total_loss = loss + penalty\n","        \n","        total_loss.backward()\n","        optimizer.step()\n","        \n","        if i % 70 == 0:\n","            print(f\"Loss: {loss.item():.4f}, Penalty: {penalty.item():.4f}, Total Loss: {total_loss.item():.4f}\")\n","    \n","    print(f\"Epoch {epoch+1} completed\")\n","    \n","    # Test\n","    correct = 0\n","    total = 0\n","    class_correct = defaultdict(int)\n","    class_total = defaultdict(int)\n","    init_model.eval()\n","    \n","    with torch.no_grad():\n","        for crop_image, class_idx in test_loader:\n","            crop_image = crop_image.cuda().float()\n","            class_idx = class_idx.cuda()\n","            outputs = init_model(crop_image)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += class_idx.size(0)\n","            correct += (predicted == class_idx).sum().item()\n","            \n","            # Per-class accuracy\n","            for pred, true in zip(predicted, class_idx):\n","                if pred == true:\n","                    class_correct[true.item()] += 1\n","                class_total[true.item()] += 1\n","\n","    print(f'Overall Accuracy of the network on the test images: {100 * correct / total:.2f}%')\n","    \n","    # Print per-class accuracy\n","    for class_idx in range(4):\n","        if class_total[class_idx] > 0:\n","            accuracy = 100 * class_correct[class_idx] / class_total[class_idx]\n","            print(f'Accuracy of class {class_idx}: {accuracy:.2f}%')\n","        else:\n","            print(f'No test samples for class {class_idx}')\n","\n","    # Print the number of samples in each class\n","    for class_idx in range(4):\n","        print(f'Number of test samples in class {class_idx}: {class_total[class_idx]}')\n","\n","    # Calculate and print false negative rate for beans\n","    false_negatives = sum(class_total[i] - class_correct[i] for i in range(3))  # Sum for all bean classes\n","    total_beans = sum(class_total[i] for i in range(3))\n","    false_negative_rate = 100 * false_negatives / total_beans if total_beans > 0 else 0\n","    print(f'False Negative Rate for Beans: {false_negative_rate:.2f}%')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# import torch\n","# import torch.nn.functional as F\n","# from collections import defaultdict\n","\n","# for epoch in range(10):\n","#     init_model.train()\n","#     for i, (crop_image, class_idx) in enumerate(train_loader):\n","#         crop_image = crop_image.cuda().float()\n","#         class_idx = class_idx.cuda()\n","#         optimizer.zero_grad()\n","#         output = init_model(crop_image)\n","#         loss = F.cross_entropy(output, class_idx)\n","#         loss.backward()\n","#         optimizer.step()\n","#         if i % 10 == 0:\n","#             print(\"Loss: \", loss.item())\n","#     print(f\"Epoch {epoch+1} completed\")\n","    \n","#     # Test\n","#     correct = 0\n","#     total = 0\n","#     class_correct = defaultdict(int)\n","#     class_total = defaultdict(int)\n","#     init_model.eval()\n","    \n","#     with torch.no_grad():\n","#         for crop_image, class_idx in test_loader:\n","#             crop_image = crop_image.cuda().float()\n","#             class_idx = class_idx.cuda()\n","#             outputs = init_model(crop_image)\n","#             _, predicted = torch.max(outputs.data, 1)\n","#             total += class_idx.size(0)\n","#             correct += (predicted == class_idx).sum().item()\n","            \n","#             # Per-class accuracy\n","#             for pred, true in zip(predicted, class_idx):\n","#                 if pred == true:\n","#                     class_correct[true.item()] += 1\n","#                 class_total[true.item()] += 1\n","\n","#     print('Overall Accuracy of the network on the test images: %d %%' % (100 * correct / total))\n","    \n","#     # Print per-class accuracy\n","#     for class_idx in range(4):  # Assuming 3 classes (0, 1, 2)\n","#         if class_total[class_idx] > 0:\n","#             accuracy = 100 * class_correct[class_idx] / class_total[class_idx]\n","#             print(f'Accuracy of class {class_idx}: {accuracy:.2f}%')\n","#         else:\n","#             print(f'No test samples for class {class_idx}')\n","\n","#     # Print the number of samples in each class\n","#     for class_idx in range(4):\n","#         print(f'Number of test samples in class {class_idx}: {class_total[class_idx]}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import logging\n","from ultralytics.utils import LOGGER\n","LOGGER.setLevel(logging.ERROR)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%time\n","import cv2\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from ultralytics import YOLO\n","from scipy.cluster.hierarchy import linkage, fcluster\n","\n","def apply_clahe(img):\n","    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n","    l, a, b = cv2.split(lab)\n","    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n","    cl = clahe.apply(l)\n","    limg = cv2.merge((cl,a,b))\n","    return cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n","\n","def adjust_gamma(img, gamma=1.0):\n","    inv_gamma = 1.0 / gamma\n","    table = np.array([((i / 255.0) ** inv_gamma) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n","    return cv2.LUT(img, table)\n","\n","def non_max_suppression_fast(boxes, scores, overlapThresh):\n","    if len(boxes) == 0:\n","        return []\n","\n","    if boxes.dtype.kind == \"i\":\n","        boxes = boxes.astype(\"float\")\n","\n","    pick = []\n","    x1, y1, x2, y2 = boxes[:,0], boxes[:,1], boxes[:,2], boxes[:,3]\n","    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n","    idxs = np.argsort(scores)\n","\n","    while len(idxs) > 0:\n","        last = len(idxs) - 1\n","        i = idxs[last]\n","        pick.append(i)\n","\n","        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n","        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n","        xx2 = np.minimum(x2[i], x2[idxs[:last]])\n","        yy2 = np.minimum(y2[i], y2[idxs[:last]])\n","\n","        w = np.maximum(0, xx2 - xx1 + 1)\n","        h = np.maximum(0, yy2 - yy1 + 1)\n","\n","        overlap = (w * h) / area[idxs[:last]]\n","        idxs = np.delete(idxs, np.concatenate(([last], np.where(overlap > overlapThresh)[0])))\n","\n","    return pick\n"," \n","def filter_detections_by_size(detections, std_threshold=3):\n","    # Calculate areas of all detections\n","    areas = [abs(d['box'][2] - d['box'][0]) * abs(d['box'][3] - d['box'][1]) for d in detections]\n","    print(\"non-filtered_detections:\", len(detections))\n","    # Calculate mean and standard deviation of areas\n","    mean_area = np.mean(areas)\n","    std_area = np.std(areas)\n","    max_area = np.max(areas)\n","    min_area = np.min(areas)\n","\n","    print(\"Mean area:\", mean_area)\n","    print(\"Std area:\", std_area)\n","    print(\"Max area:\", max_area)\n","    print(\"Min area:\", min_area)\n","    \n","    # Define acceptable range\n","    min_area = 200\n","    max_area = (mean_area*std_threshold)\n","    max_area = max(max_area, 20000)\n","\n","    print(\"Min area:\", min_area)\n","    print(\"Max area:\", max_area)\n","    \n","    # Filter detections\n","    filtered_detections = [\n","        d for d, area in zip(detections, areas) \n","        if min_area <= area <= max_area\n","    ]\n","\n","    print(\"filtered_detections:\", len(filtered_detections))\n","    \n","    return filtered_detections\n","\n","def efficient_multi_scale_detection(image_path, model_path, conf_threshold=0.1, iou_threshold=0.3):\n","    model = YOLO(model_path)\n","    original_image = cv2.imread(image_path)\n","    h, w = original_image.shape[:2]\n","\n","    scale_factors = [1.0, 1.5]\n","    patch_sizes = [(h//1, w//1), (h//2, w//2)]\n","\n","    all_detections = []\n","    batch_inserts= []\n","    for scale in scale_factors:\n","        scaled_image = cv2.resize(original_image, (int(w*scale), int(h*scale)))\n","        \n","        enhanced_images = [\n","            scaled_image,\n","            apply_clahe(scaled_image),\n","            adjust_gamma(scaled_image, 0.8),\n","            adjust_gamma(scaled_image, 1.2)\n","        ]\n","\n","        for img in enhanced_images:\n","            batch_inserts.append((img, scale, (0,0)))\n","\n","            for patch_h, patch_w in patch_sizes:\n","                for i in range(0, img.shape[0] - patch_h + 1, patch_h // 2):\n","                    for j in range(0, img.shape[1] - patch_w + 1, patch_w // 2):\n","                        patch = img[i:i+patch_h, j:j+patch_w]\n","                        batch_inserts.append((patch, scale, (i, j)))\n","\n","\n","\n","    results = model([v[0] for v in batch_inserts], conf=conf_threshold)\n","    for details, r in zip(batch_inserts,results):\n","        img, scale, (i, j) = details\n","        boxes = r.boxes.xyxy.cpu().numpy()\n","        scores = r.boxes.conf.cpu().numpy()\n","        for box, score in zip(boxes, scores):\n","            adjusted_box = [\n","                (box[0] + j) / scale,\n","                (box[1] + i) / scale,\n","                (box[2] + j) / scale,\n","                (box[3] + i) / scale\n","            ]\n","            all_detections.append({\n","                'box': adjusted_box,\n","                'score': score,\n","                'scale': scale\n","            })\n","            \n","    print(\"All detections:\", len(all_detections))\n","    all_detections = filter_detections_by_size(all_detections)\n","    boxes = np.array([d['box'] for d in all_detections])\n","    scores = np.array([d['score'] for d in all_detections])\n","\n","    \n","    keep = non_max_suppression_fast(boxes, scores, iou_threshold)\n","    \n","    filtered_detections = [all_detections[i] for i in keep]\n","\n","    if len(filtered_detections) == 0:\n","        print(\"Nothing detected\")\n","        return [], original_image\n","\n","    detection_features = np.array([[d['box'][0], d['box'][1], d['box'][2], d['box'][3], d['score'], d['scale']] for d in filtered_detections])\n","    Z = linkage(detection_features, 'ward')\n","    # final_results = [all_detections[i] for i in keep]\n","\n","    # final_results.sort(key=lambda x: x['score'], reverse=True)\n","    clusters = fcluster(Z, t=0.5, criterion='distance')\n","\n","    final_results = []\n","    for cluster_id in np.unique(clusters):\n","        cluster_detections = [filtered_detections[i] for i in range(len(filtered_detections)) if clusters[i] == cluster_id]\n","        best_detection = max(cluster_detections, key=lambda x: x['score'])\n","        final_results.append(best_detection)\n","\n","    final_results.sort(key=lambda x: x['score'], reverse=True)\n","\n","    return final_results, original_image\n","\n","def merge_close_boundaries(detections, iou_threshold=0.7):\n","    merged = []\n","    detections.sort(key=lambda x: x['score'], reverse=True)\n","    \n","    for det in detections:\n","        if not merged:\n","            merged.append(det)\n","        else:\n","            should_merge = False\n","            for i, m_det in enumerate(merged):\n","                iou = calculate_iou(det['box'], m_det['box'])\n","                if iou > iou_threshold:\n","                    merged[i]['box'] = [\n","                        min(det['box'][0], m_det['box'][0]),\n","                        min(det['box'][1], m_det['box'][1]),\n","                        max(det['box'][2], m_det['box'][2]),\n","                        max(det['box'][3], m_det['box'][3])\n","                    ]\n","                    merged[i]['score'] = max(det['score'], m_det['score'])\n","                    should_merge = True\n","                    break\n","            if not should_merge:\n","                merged.append(det)\n","    \n","    return merged\n","\n","def calculate_iou(box1, box2):\n","    x1 = max(box1[0], box2[0])\n","    y1 = max(box1[1], box2[1])\n","    x2 = min(box1[2], box2[2])\n","    y2 = min(box1[3], box2[3])\n","    \n","    intersection = max(0, x2 - x1) * max(0, y2 - y1)\n","    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n","    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n","    \n","    iou = intersection / float(area1 + area2 - intersection)\n","    return iou\n","\n","\n","\n","# Usage\n","# if __name__ == '__main__':\n","image_path = \"/home/ubuntu/technoserve/2024-07-05 11.28.08.jpg\"\n","model_path = 'runs/detect/train17/weights/best.pt'\n","detections, original_image = efficient_multi_scale_detection(image_path, model_path)\n","\n","# Merge close boundaries\n","detections = merge_close_boundaries(detections)\n","detections = filter_detections_by_size(detections)\n","\n","# Visualize results\n","result_image = original_image.copy()\n","images = []\n","for det in detections:\n","    box = det['box']\n","    score = det['score']\n","    area = abs(box[2] - box[0]) * abs(box[3] - box[1])\n","    images.append((original_image[int(box[1]):int(box[3]), int(box[0]):int(box[2])], box, score))\n","    cv2.rectangle(result_image, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (0, 255, 0), 2)\n","    # cv2.putText(result_image, f\"{area:.2f}\", (int(box[0]), int(box[1])-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,255,0), 2)\n","print (\"Detection: \", len(images))\n","# plot the image\n","plt.figure(figsize=(12, 12))\n","plt.imshow(cv2.cvtColor(result_image, cv2.COLOR_BGR2RGB))\n","plt.axis('off')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["images = [(pre_process_image(img, False), box, score)  for img, box, score in images]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(images)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Predict classes\n","init_model.eval()\n","batched_images = torch.stack([torch.Tensor(img) for img, _, _ in images])\n","with torch.no_grad():\n","    outputs = init_model(batched_images.cuda().float())\n","    # get variance of the output\n","    variance = torch.var(outputs, dim=1)\n","    _, predicted_clsx = torch.max(outputs.data, 1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["predicted_clsx.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Visualize results\n","result_image = original_image.copy()\n","total_detection = len(images)\n","classes_count = [0, 0, 0, 0]\n","for i, (_, box, score) in enumerate(images):\n","    colrs = [\n","        (0, 255, 0),\n","        (255, 0, 0),\n","        (0, 0, 255),\n","        (0,0,0)\n","    ]\n","    # curr_score = (variance[i] - variance.min()) / (variance.max() - variance.min())\n","    sel_color = colrs[predicted_clsx[i].item()]\n","    classes_count[predicted_clsx[i].item()] += 1\n","    if not (score >= 0.05):\n","        sel_color = (0, 0, 0)\n","    cv2.rectangle(result_image, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])),sel_color , 2)\n","\n","plt.figure(figsize=(12, 12))\n","plt.imshow(cv2.cvtColor(result_image, cv2.COLOR_BGR2RGB))\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i in range(4):\n","    prcent = (classes_count[i] / total_detection) * 100\n","    class_namse = [\"Overripe\", \"Ripe\", \"Underripe\", \"nan\"]\n","\n","    print(f\"Class {class_namse[i]}: {prcent:.2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# draw histogram of sizes of images by the class\n","import matplotlib.pyplot as plt\n","from collections import defaultdict\n","\n","class_sizes = defaultdict(list)\n","max_width = 0\n","min_width = 100000\n","max_height = 0\n","min_height = 100000\n","for data in train_dataset:\n","    image, class_idx = data\n","    class_sizes[class_idx].append(image.shape[0] * image.shape[1])\n","    max_width = max(max_width, image.shape[1])\n","    min_width = min(min_width, image.shape[1])\n","    max_height = max(max_height, image.shape[0])\n","    min_height = min(min_height, image.shape[0])\n","    \n","\n","plt.figure(figsize=(12, 6))\n","plt.hist([size for size in class_sizes.values()], bins=50, label=[f\"Class {class_idx}\" for class_idx in class_sizes])\n","plt.xlabel(\"Image Size\")\n","plt.ylabel(\"Frequency\")\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import glob\n","import os\n","import cv2\n","import tqdm\n","from multiprocessing import Pool\n","from PIL import Image\n","imgfls = glob.glob(\"/mnt/rd/celebpic/pictures/**/*.jpg\")\n","\n","# multiprocessing pool to check if the images are valid , else delete them\n","def check_image(imgfl):\n","    try:\n","        img = Image.open(imgfl)\n","        img.verify()\n","        img.convert(\"RGB\")\n","        \n","    except:\n","        os.remove(imgfl)\n","\n","with Pool(24) as p:\n","    list(tqdm.tqdm(p.imap(check_image, imgfls), total=len(imgfls)))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":4}
